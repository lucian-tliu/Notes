# 最小二乘法

在科学研究中，常会遇到一类问题：我们有一组实验测量数据 $(x_i,y_i), i=1,2,\ldots,n$，它们满足一个已知的函数关系 $y=f(x;\boldsymbol{\beta})$，我们需要确定参数 $\boldsymbol{\beta}$ 的最佳取值。高斯提出，最佳的参数取值应当使得下面的函数（残差）取值最小
$$
\chi^2(\boldsymbol{\beta})=\sum_{i=1}^n \frac{(y_i-f(x_i;\boldsymbol{\beta}))^2}{\sigma_i^2}
$$
其中 $\sigma_i$ 是测量数据的标准差。这种问题称为最小二乘问题。

## 线性最小二乘法

最简单的情况是函数 $f$ 关于参数 $\boldsymbol{\beta}$ 是线性的，即
$$
y=mx+b
$$

此时的残差定义为
$$
\chi^2(m,b)=\sum_{i=1}^n \frac{(y_i-(mx_i+b))^2}{\sigma_i^2}
$$

令 $\displaystyle\frac{\partial \chi^2}{\partial m}=0,\ \frac{\partial \chi^2}{\partial b}=0$，可以得到参数 $m,b$ 的解析解。
$$
m=\frac{n\sum_i \frac{x_iy_i}{\sigma_i^2}-\sum_i \frac{x_i}{\sigma_i^2}\sum_i \frac{y_i}{\sigma_i^2}}{n\sum_i \frac{x_i^2}{\sigma_i^2}-(\sum_i \frac{x_i}{\sigma_i^2})^2}
$$
$$
b=\frac{\sum_i \frac{y_i}{\sigma_i^2}-m\sum_i \frac{x_i}{\sigma_i^2}}{n}
$$

拟合参数的标准差可以通过误差传播公式计算得到
$$
\sigma_m=\sqrt{\frac{n}{n\sum_i \frac{x_i^2}{\sigma_i^2}-(\sum_i \frac{x_i}{\sigma_i^2})^2}}
$$
$$
\sigma_b=\sqrt{\frac{\sum_i \frac{x_i^2}{\sigma_i^2}}{n\sum_i \frac{x_i^2}{\sigma_i^2}-(\sum_i \frac{x_i}{\sigma_i^2})^2}}
$$

## 多项式最小二乘法

当函数变得非线性，问题会变得更加复杂。使用多项式拟合函数
$$
y=\beta_0 + \beta_1 x + \beta_2 x^2 + \ldots + \beta_m x^m
$$

残差定义为
$$
\chi^2(\boldsymbol{\beta})=\sum_{i=1}^n \frac{\left(y_i-\sum_{j=0}^m \beta_j x_i^j\right)^2}{\sigma_i^2}
$$

令
$$\displaystyle\frac{\partial \chi^2}{\partial \beta_j}=\sum_i\frac{-2\left(y_i-\sum_{k=0}^m \beta_k x_i^k\right)x_i^j}{\sigma_i^2}=0,\ j=0,1,\ldots,m$$

得到关于参数 $\beta_j$ 的 $m+1$ 个线性方程组，写成矩阵形式为
$$
\begin{pmatrix}
\sum_i \frac{1}{\sigma_i^2} & \sum_i \frac{x_i}{\sigma_i^2} & \sum_i \frac{x_i^2}{\sigma_i^2} & \ldots & \sum_i \frac{x_i^m}{\sigma_i^2} \\
\sum_i \frac{x_i}{\sigma_i^2} & \sum_i \frac{x_i^2}{\sigma_i^2} & \sum_i \frac{x_i^3}{\sigma_i^2} & \ldots & \sum_i \frac{x_i^{m+1}}{\sigma_i^2} \\
\sum_i \frac{x_i^2}{\sigma_i^2} & \sum_i \frac{x_i^3}{\sigma_i^2} & \sum_i \frac{x_i^4}{\sigma_i^2} & \ldots & \sum_i \frac{x_i^{m+2}}{\sigma_i^2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\sum_i \frac{x_i^m}{\sigma_i^2} & \sum_i \frac{x_i^{m+1}}{\sigma_i^2} & \sum_i \frac{x_i^{m+2}}{\sigma_i^2} & \ldots & \sum_i \frac{x_i^{2m}}{\sigma_i^2}
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_m
\end{pmatrix}=
\begin{pmatrix}
\sum_i \frac{y_i}{\sigma_i^2} \\
\sum_i \frac{x_i y_i}{\sigma_i^2} \\
\sum_i \frac{x_i^2 y_i}{\sigma_i^2} \\
\vdots \\
\sum_i \frac{x_i^m y_i}{\sigma_i^2}
\end{pmatrix}
$$

系数矩阵是一个对称矩阵，当所有的 $x_i$ 都是分离的时，它是非奇异的，因此可以解出参数 $\beta_j$。但当阶数 $m$ 较大时，矩阵会变得病态，从而导致数值不稳定。例如 Hilbert 矩阵。

## 正交多项式拟合

$n$ 阶多项式可以看作是以 $(1,x,x^2,\ldots,x^n)$ 为基底的函数空间中的一个点 $\boldsymbol{\beta}=(\beta_0,\beta_1,\ldots,\beta_n)$。如果定义内积
$$
\langle f,g \rangle = \sum_{i=1}^m \frac{f(x_i)g(x_i)}{\sigma_i^2}
$$
设 $\phi_0=1,\ \phi_1=x,\ \phi_2=x^2,\ \ldots,\ \phi_n=x^n$，最小二乘法公式变为
$$
\sum_{i=0}^n \langle \phi_j, \phi_i \rangle \beta_i = \langle y, \phi_j \rangle,\ j=0,1,\ldots,n
$$  
如果选取一组正交多项式 $\{P_0(x), P_1(x), \ldots, P_n(x)\}$ 作为基底，使得
$$
\langle P_j, P_i \rangle = 0,\quad j \neq i
$$
则最小二乘法公式的系数矩阵变为对角矩阵，求解过程大大简化，拟合参数为
$$
\beta_j = \frac{\langle y, P_j \rangle}{\langle P_j, P_j \rangle},\ j=0,1,\ldots,n
$$

## 非线性最小二乘法

当函数 $f$ 关于参数 $\boldsymbol{\beta}$ 是非线性的，无法通过解析方法求解时，则使用迭代数值方法来最小化残差函数 $\chi^2(\boldsymbol{\beta})$。现在最常用的方法是 Levenberg-Marquardt 方法。

### 牛顿高斯法
牛顿高斯法是基于牛顿法的一种非线性最小二乘优化方法。它通过线性化残差函数来迭代更新参数。设残差向量为
$$
\mathbf{r}(\boldsymbol{\beta}) = \mathbf{y} - \mathbf{f}(\mathbf{x}; \boldsymbol{\beta})
$$

优化的目标函数是
$$
S(\boldsymbol{\beta}) = \frac{1}{2} \mathbf{r}(\boldsymbol{\beta})^T \mathbf{W} \mathbf{r}(\boldsymbol{\beta})
$$
其中 $\mathbf{W}$ 是权重矩阵，通常为对角矩阵，元素为 $\frac{1}{\sigma_i^2}$。假设在当前参数估计为 $\boldsymbol{\beta}_k$ ，利用泰勒展开对残差进行线性化
$$
\mathbf{r}(\boldsymbol{\beta}) \approx \mathbf{r}(\boldsymbol{\beta}_k) + \mathbf{J}(\boldsymbol{\beta}_k)(\boldsymbol{\beta} - \boldsymbol{\beta}_k)
$$
其中 $\mathbf{J}(\boldsymbol{\beta}_k)$ 是残差的雅可比矩阵，定义为
$$
\mathbf{J}_{ij} = \frac{\partial r_i(\boldsymbol{\beta})}{\partial \beta_j}
$$
代入目标函数，得到
$$
S(\boldsymbol{\beta}) \approx \frac{1}{2} \left[ \mathbf{r}(\boldsymbol{\beta}_k) + \mathbf{J}(\boldsymbol{\beta}_k)(\boldsymbol{\beta} - \boldsymbol{\beta}_k) \right]^T \mathbf{W} \left[ \mathbf{r}(\boldsymbol{\beta}_k) + \mathbf{J}(\boldsymbol{\beta}_k)(\boldsymbol{\beta} - \boldsymbol{\beta}_k) \right]
$$
令 $\Delta \boldsymbol{\beta} = \boldsymbol{\beta} - \boldsymbol{\beta}_k$，对 $S(\boldsymbol{\beta})$ 关于 $\Delta \boldsymbol{\beta}$ 求导并令其为零，得到参数更新方程
$$
\left[ \mathbf{J}(\boldsymbol{\beta}_k)^T \mathbf{W} \mathbf{J}(\boldsymbol{\beta}_k) \right] \Delta \boldsymbol{\beta} = -\mathbf{J}(\boldsymbol{\beta}_k)^T \mathbf{W} \mathbf{r}(\boldsymbol{\beta}_k)
$$
更新参数
$$
\boldsymbol{\beta}_{k+1} = \boldsymbol{\beta}_k + \Delta \boldsymbol{\beta}
$$

### Levenberg-Marquardt 方法

Levenberg-Marquardt 方法结合了梯度下降法和牛顿法的优点，通过引入阻尼因子 $\lambda$ 来调整参数更新的步长。其更新方程为
$$
\left[ \mathbf{J}(\boldsymbol{\beta}_k)^T \mathbf{W} \mathbf{J}(\boldsymbol{\beta}_k) + \lambda \mathbf{I} \right] \Delta \boldsymbol{\beta} = -\mathbf{J}(\boldsymbol{\beta}_k)^T \mathbf{W} \mathbf{r}(\boldsymbol{\beta}_k)
$$

具体的迭代步骤为：

1. 初始化参数 $\boldsymbol{\beta}_0$ 和阻尼因子 $\lambda$（通常取 $10^{-3}\sim 10^{-1}$）。设置增长因子 $\nu > 1$。
2. 计算残差 $\mathbf{r}(\boldsymbol{\beta}_k)$ 和雅可比矩阵 $\mathbf{J}(\boldsymbol{\beta}_k)$。
3. 求解更新方程，得到 $\Delta \boldsymbol{\beta}$。
4. 评估新的参数 $\boldsymbol{\beta}_{\text{new}} = \boldsymbol{\beta}_k + \Delta \boldsymbol{\beta}$，计算新的残差 $S(\boldsymbol{\beta}_{\text{new}})$。
   - 如果 $S(\boldsymbol{\beta}_{\text{new}}) < S(\boldsymbol{\beta}_k)$，则接受更新，设置 $\boldsymbol{\beta}_{k+1} = \boldsymbol{\beta}_{\text{new}}$，并减小阻尼因子 $\lambda = \lambda / \nu$。
   - 否则，拒绝更新，保持 $\boldsymbol{\beta}_{k+1} = \boldsymbol{\beta}_k$，并增大阻尼因子 $\lambda = \lambda \cdot \nu$。
5. 重复步骤 2-4，直到满足收敛条件。

# 插值近似

插值是一种通过已知数据点来估计未知数据点的方法。

## 多项式插值

对于给定的 $n+1$ 个数据点 $(x_0, y_0), (x_1, y_1), \ldots, (x_n, y_n)$，可以构造一个 $n$ 次多项式 $P(x)$
$$
P(x) = a_0 + a_1 x + a_2 x^2 + \ldots + a_n x^n
$$
使得 $P(x_i) = y_i$，$i=0,1,\ldots,n$。可以证明，只用所有数据点都是分离的，则插值多项式是唯一存在的。

### 拉格朗日插值
拉格朗日插值的方法是构造一组多项式基函数 $\{L_0(x), L_1(x), \ldots, L_n(x)\}$，使得
$$
L_i(x_j) = \delta_{ij}
$$
插值多项式可以表示为
$$
P(x) = \sum_{i=0}^n y_i L_i(x)
$$
这组基函数可以表示为
$$
L_i(x) = \prod_{\substack{0 \leq j \leq n \\ j \neq i}} \frac{x - x_j}{x_i - x_j}=\frac{(x - x_0)(x - x_1) \cdots (x - x_{i-1})(x - x_{i+1}) \cdots (x - x_n)}{(x_i - x_0)(x_i - x_1) \cdots (x_i - x_{i-1})(x_i - x_{i+1}) \cdots (x_i - x_n)}
$$

### 牛顿插值

牛顿插值使用差商（Divided Differences）来构造插值多项式。先看一个数据点的情况
$$
P_0(x) = y_0
$$
因此定义零阶差商为
$$
f[x_0] = y_0
$$
对于两个数据点 $(x_0, y_0), (x_1, y_1)$，插值多项式为
$$
P_1(x) = f[x_0] + f[x_0, x_1](x - x_0)
$$
其中一阶差商定义为
$$
f[x_0, x_1] = \frac{f[x_1] - f[x_0]}{x_1 - x_0} = \frac{y_1 - y_0}{x_1 - x_0}
$$
对于 $n+1$ 个数据点，插值多项式为
$$
P_n(x) = f[x_0] + f[x_0, x_1](x - x_0) + f[x_0, x_1, x_2](x - x_0)(x - x_1) + \ldots + f[x_0, x_1, \ldots, x_n](x - x_0)(x - x_1) \cdots (x - x_{n-1})
$$
其中 $k$ 阶差商定义为
$$
f[x_0, x_1, \ldots, x_k] = \frac{f[x_1, x_2, \ldots, x_k] - f[x_0, x_1, \ldots, x_{k-1}]}{x_k - x_0}
$$

>[!caution]
>
> 高阶多项式插值在边界处可能会出现龙格现象（Runge's Phenomenon），因此一般对于需要估计的数据点应放在插值区域中间。

## 厄米插值

厄米插值（Hermite Interpolation）不仅要求插值多项式在数据点处通过给定的函数值，还要求其导数值也与给定的导数值相匹配。

设有 $n+1$ 个数据点 $(x_0, y_0, y_0'), (x_1, y_1, y_1'), \ldots, (x_n, y_n, y_n')$。厄米插值多项式 $H(x)$ 满足
$$
H(x_i) = y_i,\quad H'(x_i) = y_i',\quad i=0,1,\ldots,n
$$
厄米插值多项式可以表示为
$$
H(x) = \sum_{i=0}^n \left[ y_i h_i^2(x) + y_i' h_i^1(x) \right]
$$
其中基函数 $h_i^2(x)$ 和 $h_i^1(x)$ 定义为
$$
h_i^2(x) = (1 - 2(x - x_i)L_i'(x_i))L_i^2(x)
$$
$$
h_i^1(x) = (x - x_i)L_i^2(x)
$$

## 三次样条插值

三次样条插值是一种分段多项式插值方法，它在每个子区间上使用三次多项式进行插值，并确保在节点处函数值及其一阶和二阶导数连续。
设有 $n+1$ 个数据点 $(x_0, y_0), (x_1, y_1), \ldots, (x_n, y_n)$，三次样条插值在每个区间 $[x_i, x_{i+1}]$ 上定义一个三次多项式
$$S_i(x) = a_i + b_i (x - x_i) + c_i (x - x_i)^2 + d_i (x - x_i)^3,\quad i=0,1,\ldots,n-1$$
要求满足以下条件：

1. 插值条件：$S_i(x_i) = y_i$ 和 $S_i(x_{i+1}) = y_{i+1}$，确保多项式通过所有数据点。
2. 一阶连续性：$S_i'(x_{i+1}) = S_{i+1}'(x_{i+1})$，确保在节点处一阶导数连续。
3. 二阶连续性：$S_i''(x_{i+1}) = S_{i+1}''(x_{i+1})$，确保在节点处二阶导数连续。
4. 边界条件：可以选择自然边界条件（$S_0''(x_0) = 0$ 和 $S_{n-1}''(x_n) = 0$）或其他类型的边界条件。

通过上述条件，可以建立一个线性方程组来求解系数 $a_i, b_i, c_i, d_i$。最终得到的三次样条函数 $S(x)$ 在每个区间上由对应的 $S_i(x)$ 定义。

**一般直接利用现有库函数计算**

