# 基础知识

## 1 Introduction

机器学习的本质是根据需要**寻找函数**，典型的函数类型有：

-   **回归**(regression)：输出一个标量的函数

    ```mermaid
    flowchart LR
        A[PM2.5 today<br/>temperature<br/>concentration of O3] --> B[f]
        B --> C[PM2.5 of tomorrow]
    
        style B fill:#4a7fe7,stroke:#333,stroke-width:1px,color:#fff
    
    ```

-   **分类**(classification)：给定一组选项（类别 (classes)），能够输出正确选项的函数

    ```mermaid
    flowchart LR
        A[An email] --> B[f]
        B --> C[Junk mail or not]
    
        style B fill:#4a7fe7,stroke:#333,stroke-width:1px,color:#fff
    ```

实际上，回归和分类只是机器学习的一小部分，它们的共同特点是输出量比较简单。对于更复杂，具有结构特征的输出，比如图像、文档等，我们把这种情况称为**结构化学习**(structured learning)。

```mermaid
flowchart LR
    A[voice] --> B[f]
    B --> C[text]
    
    A1[text] --> B[f]
    
    B --> D[image]
    
    A2[……] --> B
    B --> E[……]
    style B fill:#4a7fe7,stroke:#333,stroke-width:1px,color:#fff
```

## 2 Training

机器学习的核心：**寻找函数函数**的步骤，这些步骤称为**训练(training)**。一般训练分为三个步骤：

```mermaid
flowchart LR
	A[function with <br> unknown parameters] 
	--> B[define <br> loss function <br> from training data]
	
	B --> C[optimization]
	
	style A fill:#1f77b4,stroke:#333,stroke-width:1px,color:#fff
	style B fill:grey,stroke:#333,stroke-width:1px,color:#fff
	style C fill:#17becf,stroke:#333,stroke-width:1px,color:#fff
```

### 2.1 构造带有未知参数的函数

-   根据领域知识构造函数，构造的函数称为**模型(model)**

-   以简单的线性模型为例
    $$
    y=b+wx
    $$
    其中已知的 $x$ 称为**特征(feature)**，未知参数 $w$ 和 $b$ 分别称为**权重(weight)**和**偏移(bais)**

### 2.2 定义损失函数

**损失(Loss)**：关于未知参数的函数，一般写作 $L(w,b)$，用来评估模型和好坏。

-   对于一组 $(w,b)$ 的值，代入已知数据 $x$，计算出预测值 $y$；并且记真实值为 $\hat{y}$（称为**标签**(label)），那么对应的误差
    -   绝对误差：$e=|y-\hat{y}|$
    -   平方误差：$e=(y-\hat{y})^2$

-   对于总共 $N$ 个数据，一般采用误差的均值作为损失
    $$
    L=\frac{1}{N}\sum_i e_i
    $$

-   如果 $y,\hat{y}$ 是按概率分布的，那么常用[交叉熵](https://en.wikipedia.org/wiki/Cross-entropy) (cross-entropy) 计算

对于简单的模型，我们可以直接对参数做搜索，计算损失 $L$，得到一张误差曲面(error surface)

<img src=".\figure\error_surface.png" style="zoom:40%;" />

### 2.3 优化

优化(optimization)的目标是找到一组 $(w^*,b^*)$ 使得损失最小，即
$$
w^*,b^*=\arg\big(\min_{w,b} L\big)
$$

#### 2.3.1 梯度下降法

最常使用的方法是**梯度下降法(gradient descent)**。以最简单的单参数优化为例：对于函数 $L(w)$

-   寻找一个初始值 $w_0$。注意初始化过程并不随意，[参数初始化可以防止梯度消失和梯度爆炸](http://www.deeplearning.ai/ai-notes/initialization/?utm_source=email&utm_medium=newsletter&utm_campaign=BlogAINotesInTextMay082019)
-   计算梯度 $\displaystyle g(w_0)=\left.\frac{\partial L}{\partial w}\right|_{w=w_0}$
-   更新参数 $w\longleftarrow w_0+\eta g(w_0)$。其中 $\eta$ 称为学习速率(learning rate)，是一个很小的参数，事先人为确定（超参数）
-   迭代上述步骤，直到
    -   找到最小值（全局/局域）
    -   达到最大迭代次数

下面对一个简单的损失函数 $L(x)=x^2$ 上，采用不同的学习速率展示梯度下降的过程

<img src=".\figure\gradient_dec.png" style="zoom:50%;" />

-   学习率比较小，可以成功找到极小值点
-   学习率比较大，参数左右跳动，表现为训练、验证误差震荡
-   学习率太大，导致发散

前面我们提到对于多个极值的损失函数，梯度下降可能找到局部最小值，这不是我们期望的

<img src=".\figure\local.png" style="zoom:20%;" />

现在将参数扩展为二维，考虑 $\displaystyle L(x_1,x_2)=0.1{x_1}^2+2{x_2}^2$，直接使用梯度下降法

<img src=".\figure\2dsvd.png" style="zoom:24%;" />

-   学习率较小时，难以在有限步数达到最小值
-   学习率较大时，出现严重发散

可见原始的梯度下降算法(SGD)有两个缺点：

-   学习率不好选择：太小则训练慢，太大则严重发散
-   梯度为 0 就停止训练，容易停在局部最小值

#### 2.3.2 Momentum

从物理的角度来理解，如果将损失函数看作某种场，**SGD相当于是用加速度更新位置**。但实际的物理中，应该用速度更新位置，加速度更新速度。因此可以在优化过程中引入动量(momentum)机制。

**带动量的梯度下降**
$$
v_{n}=\gamma v_{n-1}+(1-\gamma)\nabla_{\theta} L(\theta)\\
\theta_{n}=\theta_{n-1}-v_{n}
$$
在动量机制下，就算一开始的学习率比较小，因为速度v是加速度（梯度）的逐步累加，所以参数更新很快。同时由于小球有惯性，在梯度为 0 的点也不会立刻停下，会有一定的几率逃出局部极小值或鞍点。

<img src=".\figure\momentum.png" style="zoom:30%;" />

-   引入中等动量，原来会发散的学习率能够正确找到最小值
-   引入过大动量，则会引入新的震荡（小球在谷底因惯性导致的左右摇摆）

一般选择较小的学习率和较大的动量，能让损失快速稳定收敛

<img src=".\figure\momentum1.png" style="zoom:20%;" />

#### 2.3.3 RMSProp

SGD 的一个缺点是步长对各个方向都是同一个常数。这就导致在梯度大的方向更新快，梯度小的方向更新慢。引起在梯度大的方向的剧烈震荡。适配步长算法对更新快的方向使用小学习率，更新慢的方向使用大学习率，实现更光滑的更新。

**RMSProp**
$$
g(\theta) = \nabla_\theta L(\theta)\\
E = \gamma E+(1-\gamma)g^2(\theta)\\
\theta = \theta-\frac{\eta}{\sqrt{E+\epsilon}}g(\theta)
$$
其中 $E$ 是梯度平方的滑动平均，$\epsilon$ 则用来防止除零错误

<img src="D:\Notes\深度学习\figure\rmsprop.png" style="zoom:20%;" />

#### 2.3.4 Adam

动量机制与适配步长是两种完全不同的思路，Adam 将两者的优点合二为一。更新公式为
$$
\theta = \theta-\frac{\hat{m}}{\sqrt{\hat{n}}+\epsilon}\\
m_t=\beta_1m_{t-1}+(1-\beta_1)g(\theta)\\
n_t=\beta_2n_{t-1}+(1-\beta_2)g^2(\theta)\\
\hat{m}=\frac{m_t}{1-{\beta_1}^t},\quad \hat{n}=\frac{n_t}{1-{\beta_2}^t}
$$
由于一开始 $m_0 = n_0 = 0$，前几步会偏小，因此 Adam 使用偏差修正 $\hat{m}$，$\hat{n}$。Adam 论文中给出默认参数

| 超参数     | 意义       | 默认值 |
| ---------- | ---------- | ------ |
| $\eta$     | 学习率     | 1e-3   |
| $\beta_1$  | 一阶矩衰减 | 0.9    |
| $\beta_2$  | 二阶矩衰减 | 0.999  |
| $\epsilon$ | 防止除零   | 1e-8   |

<img src=".\figure\adam.png" style="zoom:20%;" />

Adam 算法收敛效果非常好，且步长更为均匀。即使是非常大的学习率 Adam 的更新轨迹也只是围绕极小值点附近做S形环绕，而不是像其他算法一样直接发散。

<img src=".\figure\adam2.png" style="zoom:20%;" />

减小第一个参数 $\beta_1$，会缩小极小值附近S形曲线的环绕区域，使结果更接近极小值点。

<img src=".\figure\adam3.png" style="zoom:20%;" />

## 3 Activation Functions

除了简单的线性模型，我们还可以构造更为复杂的模型，例如
$$
y = b+\sum_{i}w_ix_i
$$
随着特征点的增多，损失会逐渐减小直到一个极限。这种模型都是**线性模型**(linear models)，是一类最简单的模型。

### 3.1 sigmoid

由于过于简单，线性模型存在很严重的局限性（称为**模型偏移**(model bias)），无法表征所有的情况。因此我们需要一些更灵活、更精密的模型。

一种常用的模型是 sigmoid 函数
$$
y = c\frac{1}{1+\mathrm{e}^{b+wx}} = c\cdot\text{sigmoid}(b+wx)=c\cdot\sigma(b+wx)
$$

>   [!note]
>
>   在论文 [Cybenko(1989)](https://link.springer.com/article/10.1007/BF02551274) 中给出了 sigmoid 函数完备性的证明，表述为:
>
>   对任意的连续函数 $f:[0,1]^n\rightarrow \mathbb{R}$ 和任意 $\epsilon>0$，存在一个神经网络
>   $$
>   f_\theta(x)=\sum_{i=1}^{N}\alpha_i\sigma(w^\mathrm{T}x+b)
>   $$
>   使得
>   $$
>   ||f-f_\theta||<\epsilon
>   $$
>   表明 sigmoid 线性组合可以逼近所有连续函数。

>[!note]
>
>   **通用逼近定理（Universal Approximation Theorem）**：使用某个激活函数构成的前馈神经网络可以以任意精度逼近任意连续函数。即**存在一层隐藏层 + 足够多的神经元，就能逼近任意连续函数 f(x)。**

改变不同的 sigmoid 可以观察到

<img src=".\figure\sigmoid.png" style="zoom:50%;" />

对于复杂的模型，线性组合应写作
$$
y=b+\sum_{i}\alpha_i\text{sigmoid}\left(b_i+\sum_{j}w_{ij}x_j\right)
$$
用图像化表示

<img src=".\figure\sigmoid1.png" style="zoom:45%;" />

再代入计算

<img src=".\figure\sigmoid2.png" style="zoom:45%;" />

完整的写成矩阵的形式
$$
\newcommand \bm \boldsymbol

\bm{y}=\bm{b}+\bm{\alpha}\sigma(\bm{c}+\bm{w}^\text{T}\bm{x})
$$

### 3.2 ReLU

除了 sigmoid 函数外，还有一类在机器学习中常见的，且与 sigmoid 十分相似的函数：**ReLU**（rectified linear unit，整流线性单元），它的函数图像如下所示：

<img src=".\figure\ReLU.png" style="zoom:45%;" />

可以看到，它的形状就是硬 sigmoid 函数去掉其中的顶边或底边；它也接收相同的参数，但是它的函数形式更为简单，就是一个 max 函数。用 ReLU 来表示任意一个模型
$$
y=b+\sum_{2i}\alpha_i\text{ReLU}\left(c_i+\sum_{j}w_{ij}x_j\right)
$$
与 sigmoid 的等价形式相比，除了将 sigmoid 换成 max 外，另一处区别在于累加和的项数扩大了一倍，这是因为 2 个 ReLU 才能转化为 1 个 sigmoid，这一点是显而易见的。

实践效果上看，ReLU 的效果更好，原因将在后面的章节中阐述。

## 4 General Framework

### 优化

-   在优化过程中，我们将所有未知参数放在一起，构成一个 $N\times 1$ 的矩阵
    $$
    \bm{\theta}=
    \left[\begin{matrix}
    \theta_1\\
    \theta_2\\
    \vdots\\
    \theta_N
    \end{matrix}\right]
    $$
    
-   在实践过程中，一般不会一次性更新所有参数，而是将这些参数**分批**(batch) 处理，因此上面的优化过程可以改写为：<img src=".\figure\batch.png" style="zoom:45%;" />
    每次对一个 batch 的更新称为一次 update，更新完所有 batchs 称为一个 epoch，对于每个时期，每批数据都会被随机打乱 (shuffle)。

### General Guide

当我们遇到一个效果不好的模型时，我们该如何改进呢？下面给出一张总体的方向图

```mermaid
flowchart LR

    A["**General<br>Guide**"] --> B(loss on training data)

    B --> L1{{large}}
    L1 --> C(model bias)
    L1 --> E(optimization)

    C --> D[make your model complex]
    E --> F[Better optimizing method]

    B --> L2{{small}}
    L2 --> G(loss on testing data)

    G --> T1{{large}}
    T1 --> H(overfitting)

    H --> J[data augmentation]
    H --> K[make your model simpler]

    T1 --> M(mismatch)

    G --> T2{{small}}
    T2 --> L[**Good**]
    
    D -.-> O(( ))
    O -. "trade-off" .-> P(Split your training data into training set<br>and validation set for model selection)

```

下面逐一分析模型表现差的原因以及改善方法

#### 模型偏移(model bais)

-   原因：模型过于简单。一个尺寸有限的模型，通过改变参数，只能形成一个有限大小的函数空间。如果真实的函数距离这个空间太远，则会出现这种模型偏移
    <img src=".\figure\model_bais.png" style="zoom:80%;" />
-   解决办法：设计更复杂的模型。例如增加特征点数目；或使用更深的神经网络

#### 优化不足

-   原因：产生较大的损失有时不一定意味着模型的问题，也可能是优化策略选的不够好，没能使得 loss 达到全局最小值。比如 SGD 方法就可能陷入局部最小值
-   解决方法：使用更强大的优化技术

>   [!note]
>
>   如何判断模型表现差是优化不足的问题？
>
>   训练一个更深的神经网络，并比较训练集上的误差。如果训练集上，更深的神经网络表现的更差，则是优化的问题

#### 过拟合

过拟合(overfitting)：在训练集上损失小，在测试集上损失大

-   一个极端的粒子，对于训练集 $\{(x_i,\hat{y}_i)\}$ 我们使用模型
    $$
    f(x)=
    \begin{cases}
    \hat{y}_i&x = x_i\\
    \text{random}&\text{otherwise}
    \end{cases}
    $$
    这个模型没有什么用处，但它确实在训练集上无损失

-   原因：假设已知真实的数据分布（也就是最理想模型应有的样子，但无法直接观测；且由于数据是离散的，所以用虚线表示）以及训练数据（曲线上的部分点）。
    <img src=".\figure\overfit1.png" style="zoom:33%;" />
    如果我们采取弹性较大的模型，则有可能使得该模型在这些训练数据点损失非常小，那么我们可能会得到以下结果：
    <img src=".\figure\overfit2.png" style="zoom:33%;" />
    由于模型的弹性够大，对于非训练数据上的其他位置，它有着很高的自由变化程度，因而测试数据上的损失就会变得很大了，如下图所示：
    <img src="D:\Notes\深度学习\figure\overfit3.png" style="zoom:33%;" />

-   解决方法：

    1.   使用更多的训练数据：这是一种简单有效的方法
         <img src="D:\Notes\深度学习\figure\overfit4.png" style="zoom:33%;" />

    2.   **数据增强**(data augmentation)：在训练数据的基础上得到一些稍作修改的副本，从而形成更多的训练数据。
         但是不要有太大或没意义的修改，比如对于一幅图像，对其左右翻转，适当缩放得到的图像都是可以的，但是上下翻转的图像就没什么实际意义了，因为通常人类不会阅读上下颠倒的图像
         <img src="D:\Notes\深度学习\figure\overfit5.png" style="zoom:50%;" />

    3.   给模型一些**限制**(constrain)，降低其弹性。具体可以通过以下方法

         -   减少未知参数，或让网络的层之间共享一些参数
             -   后者就是**卷积神经网络**(CNN) 的做法，它是**全连接**(fully-connected) 神经网络（也就是之前介绍的一般的神经网络）的特殊情况，之后会详细介绍
         -   采用更少的特征点
         -   提早结束训练 (early stopping)
         -   正则化 (regularization)
         -   dropout

         当然，限制不能太多，否则就回到了**模型偏移**上了

----

综上，我们需要权衡好模型偏移（与损失成正比）和模型的复杂性（或弹性）的关系，从而让模型在训练数据和测试数据上的损失都尽可能地小。下图展示了二者的关系：

<img src=".\figure\generalguide.png" style="zoom:45%;" />

>   [!note]
>
>   **交叉验证**：
>
>   在训练模型时，我们常会在训练集上训练，之后在测试集上测试。但由于测试集及每次测试的结果我们总是知道的，这导致可能会出现下面这种情况：我们不断地调整模型，使得训练后模型能够在测试集上有很好地表现，但在真实地使用中却表现很差，这就是模型过拟合到测试集上了。
>
>   同样考虑下面这种极端的例子
>   $$
>   f_j(x)=
>   \begin{cases}
>   \hat{y}_i&x = x_i\\
>   \text{random}&\text{otherwise}
>   \end{cases}
>   $$
>   只要我们不断地生成不同的模型，总会有某次随机到一个在测试集上恰好表现良好的模型，但它在另外一组独立的测试集上很有可能表现极差。
>
>   改进的做法进行**交叉验证(cross validation)**：本质上就是用两组独立的测试集进行测试
>   将原来的训练数据集分为两部分，一部分仍然作为训练数据，另一部分作为验证 (validation) 数据，不参与模型的训练中。在训练数据中训练出模型，放到验证数据集测试，观察损失大小，之后再在测试集上测试损失大小。选择两次平均损失最小的模型
>
>   <img src="D:\Notes\深度学习\figure\50.png" alt="img" style="zoom:45%;" />
>
>   更进一步可以采用 **N-fold 交叉验证**：
>
>   -   将训练数据集分为 $N$ 等分
>   -   分别将其中 $1$ 份作为验证数据，剩下 $N-1$ 份作为训练数据，对模型进行训练
>   -   按上面的方法对模型进行 $N$ 次训练测试，计算平均损失
>   -   比较不同模型的平均损失，取损失最小的模型，在整个训练数据集上重新训练，得到的模型用在测试数据上，这样不同的独立的测试集上的损失应该相近