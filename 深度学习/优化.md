# 优化

## 1 Critical Point

在训练模型的时候，我们可能会遇到模型的损失无法进一步减小，也就是无法继续优化的情况。以梯度下降法为例，当**梯度接近 0 **的时候就会出现这种情况。从数学的角度看，梯度为 0 一般对应下面三种情况

<img src="./assets/critcial point.png" style="zoom:40%;" />

即**局部最小值，局部最大值和鞍点**，这三种情况统称为 critical point。

> [!note]
>
> 判断三种情况的方法：将误差函数 $L(\boldsymbol{\theta})$ 在此时的参数 $\boldsymbol{\theta}'$ 处展开
> $$
> \newcommand \bm \boldsymbol
> 
> L(\bm{\theta})=L(\bm{\theta}')+(\bm{\theta}-\bm{\theta}')^{\rm{T}}\bm{g}(\bm{\theta}')+\frac{1}{2}(\bm{\theta}-\bm{\theta}')^{\rm{T}}\bm{H}(\bm{\theta}')(\bm{\theta}-\bm{\theta}')+\cdots
> $$
> 其中：$\bm{g}$ 是梯度，$\bm{H}$ 是 Hessian 矩阵
> $$
> H_{ij}=\frac{\partial ^2 L}{\partial\theta_i\partial \theta_j}
> $$
> 在 critical point 处 $\bm{g}=0$，因此
> $$
> L(\bm{\theta})-L(\bm{\theta}')\approx\frac{1}{2}(\bm{\theta}-\bm{\theta}')^{\rm{T}}\bm{H}(\bm{\theta}')(\bm{\theta}-\bm{\theta}')
> $$
> 对应三种情况：
>
> - 局部最小值：$\bm{H}$ 是正定的，即所有特征值都大于 0
> - 局部最大值：$\bm{H}$ 是负定的，即所有特征值都小于 0
> - 鞍点：$\bm{H}$ 是不定的，即特征值有正有负
>
> 对于鞍点，Hessian 矩阵还给出了继续优化的方向，即选择 $\bm{\theta}-\bm{\theta}'=c\lambda_{\text{neg}}$，其中 $c>0$，$\lambda_{\text{neg}}$ 表示 $\bm{H}$ 负的特征值对于的特征向量。

实际上，由于计算量的原因（Hessian 矩阵需要计算到二阶导数），我们在训练中一般不会计算 Hessian 矩阵。

-----

鞍点和局部最小值哪个更容易遇到？

<img src="./assets/saddle point.png" style="zoom:50%;" />

从上面的图片可以看出，在低维度一个局部最小值的点，在高维度就可能是鞍点。而我们实际训练的模型通常有较多的参数，即处于一个高维度的参数空间，这样看来，鞍点更有可能出现。

<img src="./assets/61.png" alt="img" style="zoom:33%;" />

横坐标为：Hessian 矩阵 负的特征值个数/总的特征值个数。可以看到即使我们不断的降低 Loss，也难以走到真正的局部最小值（Minimum Ratio = 1）。

## 2 Batchs

对于参数量很大的训练，我们会分批(batch)进行优化，即

<img src=".\assets\batch.png" style="zoom:33%;" />

下面我们讨论为什么要用 batchs 进行处理。对于两种极端的情况，一次处理所有数据和一次只处理一笔数据

<img src="./assets/63.png" alt="img" style="zoom:33%;" />

- 大批量：update 的时间更长，但更稳健
- 小批量：update 的时间较短，但优化的噪声较大

实际上我们训练中使用的 GPU 有很多的 cores，可以进行大规模**并行运算**，因此对于优化的时间需要重新进行考虑：

<img src="./assets/65.png" alt="img" style="zoom:40%;" />

从图中可以看到

- 一次 update 中：较大批量的时间在达到 GPU 并行运算的极限之前不会太大
- 一次 epoch 中：较小批量由于需要做多次 updates 反而时间会更长

因此小批量在优化时间上并没有优势，它实际的优势在于能确保训练模型的精度，使模型具备更好的表现。例如

<img src="./assets/66.png" alt="img" style="zoom:40%;" />

这可以从两个方面来解释

1. 小批量更不容易被困住局部最小值

   <img src="./assets/image-20260120164735224-1768898856637-10.png" alt="image-20260120164735224" style="zoom:40%;" />
   
   - 对于大批量，如果遇到了 critical point，那么就很容易卡住不动
   - 对于小批量，每一个 batch 有不同的 Loss 曲线，某一条 Loss 走到 critical point 后，在另一条 Loss 曲线上可能仍能够优化
   
2. 小批量更可能找到平坦的局部最小值

   <img src="./assets/image-20260120165634644-1768899395491-14.png" alt="image-20260120165634644" style="zoom:40%;" />
   
   - 对于右侧的尖锐的一个局部最小值，它在进行测试时表现不好，因为 Loss 曲线一点点的偏移就会较大的改变。较大批次容易进入这个最小值
   - 对于较小的批次，由于它每个 batch 的 Loss 曲线都有一定的差异，不容易训练到右侧尖锐的局部最小值，而容易进入左侧平坦的最小值

**总结：**

| Propoties                           | Small batch       | Large batch          |
| ----------------------------------- | ----------------- | -------------------- |
| Speed for one update (no parallel)  | Fast              | Slow                 |
| Speed for one epoch (with parallel) | Fast              | Fast (not too large) |
| Speef for one epoch                 | Slow              | Fast                 |
| Gradient                            | Noisy             | Stable               |
| Optimization                        | Better $\bigstar$ | Worse                |
| Generalization                      | Batter $\bigstar$ | Worse                |

## 3 Better Gradient Descent

在前一章中，我们发现单纯的梯度下降法有两个缺点：

<img src=".\assets\2dsvd.png" style="zoom:24%;" />

-   学习率不好选择：学习率太小则训练慢，学习率太大则严重发散
-   梯度为 0 就停止训练，容易停在局部最小值

下面给出常用的改善方法

### 3.1 Momentum

从物理的角度来理解，如果将损失函数看作某种场，SGD相当于是用加速度更新位置，但实际的物理中，应该**用速度更新位置，加速度更新速度**。因此可以在优化过程中引入动量(momentum)机制。

**带动量的梯度下降**
$$
v_{n}=\gamma v_{n-1}+(1-\gamma)\nabla_{\theta} L(\theta)\\
\theta_{n}=\theta_{n-1}-v_{n}
$$
在动量机制下，就算一开始的学习率比较小，因为速度v是加速度（梯度）的逐步累加，所以参数更新很快。同时由于小球有惯性，在梯度为 0 的点也不会立刻停下，会有一定的几率逃出局部极小值或鞍点。

<img src=".\assets\momentum.png" style="zoom:20%;" />

-   引入中等动量，原来会发散的学习率能够正确找到最小值
-   引入过大动量，则会引入新的震荡（小球在谷底因惯性导致的左右摇摆）

一般选择较小的学习率和较大的动量，能让损失快速稳定收敛

<img src=".\assets\momentum1.png" style="zoom:20%;" />

### 3.2 Adagrad

SGD 的一个缺点是步长对各个方向都是同一个常数。这就导致在梯度大的方向更新快，梯度小的方向更新慢。引起在梯度大的方向的剧烈震荡。Adagrad 算法对更新快的方向使用小学习率，更新慢的方向使用大学习率，实现更光滑的更新。
$$
g(\theta_t) = \nabla_\theta L(\theta_t)\\{\sigma_t}^2 =\frac{1}{t}\sum_{i=1}^{t}(g_t)^2 \\\theta_{t+1} = \theta_t-\frac{\eta}{\sqrt{{\sigma_t}^2+\epsilon}}g(\theta)
$$
$\epsilon$ 则用来防止除零错误。

<img src="./assets/image-20260120173510883-1768901711687-16.png" alt="image-20260120173510883" style="zoom:20%;" />

可以看到，引入 Adagrad 后，原来会发散的学习率也能够正确找到最小值。但是 Adagrad 算法任然存在一些问题，例如：Adagrad 在更新参数的时候会不断累积之前的梯度，那么如果在训练开始的时候，梯度就非常大的话，那么 $\sigma$ 也变得非常大，所以没走几步就走不下去了，影响训练的效果。

### 3.3 RMSProp

为了改善 Adagrad 算法的问题，RMSProp 算法中引入动态调整
$$
g(\theta) = \nabla_\theta L(\theta)\\
E = \gamma E+(1-\gamma)g^2(\theta)\\
\theta = \theta-\frac{\eta}{\sqrt{E+\epsilon}}g(\theta)
$$
通过改变 $\gamma$ 的值，可以决定当前梯度和累积梯度的权重

- $\gamma$ 大：之前累积的梯度权重更大
- $\gamma$ 小：当前的梯度权重更大

<img src="D:\Notes\深度学习\assets\rmsprop.png" style="zoom:20%;" />

### 3.4 Adam

动量机制与适配步长是两种完全不同的思路，Adam 将两者的优点合二为一。更新公式为
$$
\theta = \theta-\frac{\hat{m}}{\sqrt{\hat{n}}+\epsilon}\\
m_t=\beta_1m_{t-1}+(1-\beta_1)g(\theta)\\
n_t=\beta_2n_{t-1}+(1-\beta_2)g^2(\theta)\\
\hat{m}=\frac{m_t}{1-{\beta_1}^t},\quad \hat{n}=\frac{n_t}{1-{\beta_2}^t}
$$
由于一开始 $m_0 = n_0 = 0$，前几步会偏小，因此 Adam 使用偏差修正 $\hat{m}$，$\hat{n}$。Adam 论文中给出默认参数

| 超参数     | 意义       | 默认值 |
| ---------- | ---------- | ------ |
| $\eta$     | 学习率     | 1e-3   |
| $\beta_1$  | 一阶矩衰减 | 0.9    |
| $\beta_2$  | 二阶矩衰减 | 0.999  |
| $\epsilon$ | 防止除零   | 1e-8   |

<img src=".\assets\adam.png" style="zoom:20%;" />

Adam 算法收敛效果非常好，且步长更为均匀。即使是非常大的学习率 Adam 的更新轨迹也只是围绕极小值点附近做S形环绕，而不是像其他算法一样直接发散。

<img src=".\assets\adam2.png" style="zoom:20%;" />

减小第一个参数 $\beta_1$，会缩小极小值附近S形曲线的环绕区域，使结果更接近极小值点。

<img src=".\assets\adam3.png" style="zoom:20%;" />

### 3.5 Learning Rate Scaling

之前的 Adagrad 算法仍可能出现一些问题，例如

<img src="./assets/image-20260120190041379-1768906843391-18.png" alt="image-20260120190041379" style="zoom:20%;" />

优化路径在到达局部最小值之前和到达最小值之后会出现振荡，这是由于梯度在较小的地方不断积累，使得总的方均根近似为 0，从而使得学习率过大。为了改进这个方法，我们使用 Learning Rate Scaling 的方法，即让学习率不仅与梯度有关，还与时间有关：

1. **学习速率衰减**(learning rate decay)：由于随着训练的深入，模型逐渐靠近最合适的参数目标，因此学习速率会随迭代次数的增加而下降，例如
   $$
   \eta(t)=\frac{\eta}{\log(t)}
   $$
   在上面的例子中，使用该方法后

   <img src="./assets/image-20260121120534038.png" alt="image-20260121120534038" style="zoom:20%;" />

2. **热身**(warm up)：随着迭代次数的增加，学习速率先增后减。之所以这样设计，是因为开始时我们并不知道误差曲面的形状，我们希望模型现在初始点附近缓慢探索一段时间，之后再进行训练。
   例如：RAdam -> Adam + 热身

> [!note]
>
> 之前的所有优化方法可以总结为：
>
> <img src="./assets/13.png" alt="img" style="zoom:40%;" />
>
> $\sigma$ 和 $\bm{m}$ 都与梯度有关，但一个是标量，一个是向量，两种的效果不会相互抵消。

## 4 Batch Normalization

### 4.1 Changing Landscape

优化过程中遇到的很多问题是由于误差曲面太过崎岖，甚至对于上面的非常简单的二次型的误差曲面，由于两个方向的变换趋势相差太大（ 沿 $x_1$ 变化慢，沿 $x_2$ 变化快），优化中也可能出现各种问题。

在真实的训练中，什么情况会导致这样两个参数维度的差异呢？

<img src="./assets/image-20260126173038329.png" alt="image-20260126173038329" style="zoom:33%;" />

可以看到如果输入样本的两个维度的 Scale 差距太大，就会出现这样的误差曲面。我们希望通过某些变化能够将这两个维度变为同一量级，使得误差曲面变成右图的样子。

### 4.2 Feature Normalization 

<img src="./assets/image-20260126174156845.png" alt="image-20260126174156845" style="zoom:33%;" />

假设一个 Batch 中有 $R$ 个样本，对每个样本的第 $i$ 个维度的数据求出平均值 $\mu_i$ 和标准差 $\sigma_i$，做变量变换
$$
\tilde{x}_i^{k}=\frac{x_{i}^{k}-\mu_i}{\sigma_i}
$$
经过变换后，每个维度都变为平均值为 0，标准差为 1，所有维度的 Scale 都相同了。

### 4.3 Consider Deep

当我们的模型是一个深度学习的网络时，第一层的输出会作为第二层的输入，因此为了第二层的参数能够更到的训练，对第一层的输出也需要做 Normalization

<img src="./assets/image-20260126174658902.png" alt="image-20260126174658902" style="zoom:33%;" />

对所有测试样本第一次的输入做变换

<img src="./assets/image-20260126175455818.png" alt="image-20260126175455818" style="zoom:40%;" />

> [!note]
>
> Normalization 做在激活函数前还是后根据具体的情况。例如：激活函数是 Sigmoid，它在 0 附近更为敏感，在前面做 Normalization 将数据平均值变为 0 可能更好。

需要注意的是，做了 Normalization 之后，训练数据之间会变得**相互关联**。如果我们是在一个 Batch 中做的 Normalization，那此时模型的输入就不再是一个单独的样本，而是整个 Batch 中的所有样本，因此这个方法被称为 Batch Normalization

<img src="./assets/image-20260126175919143.png" alt="image-20260126175919143" style="zoom:40%;" />

### 4.4 When Testing

在做测试或者实际使用模型时，我们的样本量可能不足一个 Batch，那此时的平均值和标准差从哪里来？

实际上在训练过程中，会计算所有 $\mu$ 和 $\sigma$ 的动态平均值 $\overline{\mu},\ \overline{\sigma}$ 
$$
\overline{\mu}\leftarrow p\overline{\mu}+(1-p)\mu^t\\
\overline{\sigma}\leftarrow p\overline{\sigma}+(1-p)\sigma^t
$$

----
下图展现了各种方法在不同数据量的训练效果（纵轴表示 Accuracy）

<img src="./assets/image-20260126181501207.png" alt="image-20260126181501207" style="zoom:40%;" />

其中黑色曲线表示的是一般方法，红色曲线标识最基本的批量归一化。不难发现批量归一化相比一般方法能以更少数据达到相同的准确率，即收敛速度更快。而且如果学习速率更快的话，收敛得还要快。但并不是学习速率越大越好（可以看到 BN-x30 不如 BN-x5）。

**Why Batch Normalization works?**

在论文 [*How Does Batch Normalization Help Optimization?*](https://arxiv.org/abs/1805.11604) 中，作者从实验和理论上都说明了批量归一化能够改变误差曲面的样貌。另外这篇论文认为批量归一化的有效性是一种“偶然的发现”(serendipitous)。

